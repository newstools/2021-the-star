There has been a spike on misuse of social media accounts, Facebook’s latest Community Standard Enforcement Report indicates, with the firm taking action on more than 51.2 million posts. During the fourth quarter of 2020 (October-December), Facebook took action on 6.3 million pieces of bullying and harassment content on its Facebook platform. This is up from 3.5 million in quarter three (July-September), enabled by updates in its technology to detect comments. Organized hate content totaled 6.4 million pieces up from 4 million in quarter three while 26.9 million pieces of hate speech content was flagged up from 22.1 million in the previous quarter. Investment in latest technology also helped the firm take action on accounts operating in Arabic, Spanish and Portuguese. About 2.5 million pieces of suicide and self-injury content were acted upon, which is an increase from 1.3 million in Q3, due to increased reviewer capacity. On Instagram,the technology company detected and took action on five million  pieces of bullying and harassment content, up from 2.6 million in the previous quarter, due in part to updates in technology to detect comments. Organized hate pieces also went up with 308,000 being acted upon, up from 224,000 in Q3. Hate speech content flagged on Instagram was 6.6 million pieces up from 6.5 million in Q3. Suicide and self-injury content equally went up to 3.4 million pieces up from 1.3 million in the third quarter of the year , detected due to increased reviewer capacity, the firm said on Friday. Its Community Standards and Enforcement Report tracks the firm's progress and commitment to “making Facebook and Instagram safe and inclusive.” This quarterly report shares metrics on how the firm is doing at preventing and taking action on content that goes against our Community Standards, while protecting our community’s safety, privacy, dignity and authenticity. The latest report shows some positive strides towards improvements in prevalence, providing greater transparency and accountability around content moderation operations across different Facebook products. It includes metrics across 12 policies on Facebook and 10 policies on Instagram. “Our goal is to get better and more efficient at enforcing our Community Standards. We do this by increasing our use of Artificial Intelligence (AI), by prioritizing the content that could cause the most immediate, widespread, and real-world harm, and by coordinating and collaborating with outside experts”, said Kojo Boakye, Director of Public Policy, Africa. Facebook plans to share additional metrics on Instagram and add new policy categories on Facebook. Efforts are also being made to externally audit the metrics of these reports while making the data more interactive so people can understand it better. “We will continue to improve our technology and enforcement efforts to keep harmful content off of our apps,” Boakye said in a statement on Friday. The technology company prides itself of giving people the power to build communities and bring the world closer together. Its products include the Facebook app, Messenger, Instagram, WhatsApp, Oculus, Workplace, Portal, and Novi, with a user base of more than three billion people around the world.